{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c419c5a7",
   "metadata": {},
   "source": [
    "Source: https://youtu.be/ISNdQcPhsts?si=_1mO7CBcvFHg15cJ\n",
    "\n",
    "Umar Jamil has taken inspiration from the [Harvard pytorch transformer article](https://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c137f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea308e40",
   "metadata": {},
   "source": [
    "### Input Embedding\n",
    "\n",
    "TODO:\n",
    "\n",
    "- explore what `nn.Embedding` does\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d798376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.d_model = d_model  # in this paper, it 512\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "        # check the last line on page 5: \n",
    "        # \"In the embedding layers, we multiply those weights by d model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f1a93",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "TODO:\n",
    "- check Amirhossein Kazamnejad's blog on positional encoding\n",
    "\n",
    "Umar Jamil uses the [Harvard pytorch transformer article implementation of positional encoding formula](https://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding) mentioned in the paper by using log. He mentions in his video that applying log to an exponential nullifies the effect of log but makes the calculation more numerically stable. The value of the positional encoding calculated this way will be slightly different but the model will learn. Click [here](https://youtu.be/ISNdQcPhsts?si=HNaqDgkw6CfwgO-M&t=470) to watch that particular scene from the video.\n",
    "\n",
    "Click [here](https://youtu.be/ISNdQcPhsts?si=cvEfkDJyW7LiBqkn&t=720) to see the reasoning behind using `self.register_buffer(\"pe\", pe)`. The reasoning that when we want to save some variable not as a learned parameter (like weights and biases) but we want it to be saved when we save the file of the model, the we should register it as a buffer. This way it will be saved along with the state of the model.\n",
    "\n",
    "Original formula:\n",
    "\n",
    "$$PE_{(pos, 2i)} = sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)$$\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Modified formula by Harvard Transformer article:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6513109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model  # in this paper, it 512\n",
    "        self.seq_len = seq_len  # maximum length of the sequence\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # create a matrix of shape (seq_len, d_model)\n",
    "        # pe stands for positional encoding\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # create a vector of shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        # now, we will create the denominator of the positional encoding formulae\n",
    "        # since it is a bit long, we will break it into a few lines\n",
    "        # first, we need a vector containing multiples of 2 from 0 to d_model (here, 512)\n",
    "        # this line is because of the 2i term which is the power of 10000\n",
    "        # thus, this vector provides for the numbers we need for 2i\n",
    "        vector = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "        # now, we raise 10,000 to the power of 2i/d_model\n",
    "        denominator_original = torch.pow(10000, vector/d_model)\n",
    "        # this is the one used by Harvard Transformer article\n",
    "        denominator_harvard = torch.exp(vector * (-math.log(10000.0)/d_model))\n",
    "        # we apply sin for even dimension and cos for odd dimenion\n",
    "        # apply sin and store it in even indices of pe\n",
    "        pe[:, 0::2] = torch.sin(position * denominator_original)\n",
    "        # apply cos and store it in odd indices of pe\n",
    "        pe[:, 1::2] = torch.cos(position * denominator_original)\n",
    "        # we need to add the batch dimension so that we can apply it to \n",
    "        # batches of sentences\n",
    "        pe = pe.unsqueeze(0)  # new shape: (1, seq_len, d_model)\n",
    "        # register the pe tensor as a buffer so that it can be saved along with the\n",
    "        # state of the model\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe1531",
   "metadata": {},
   "source": [
    "Let's see how the positional encoding works by doing it on a smaller example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1090d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0264,  0.9997,  0.8573, -0.5148, -0.1383,  0.9904,\n",
      "          0.9992,  0.0402],\n",
      "        [ 0.9093, -0.4161,  0.0528,  0.9986, -0.8827, -0.4699, -0.2739,  0.9618,\n",
      "          0.0803, -0.9968],\n",
      "        [ 0.1411, -0.9900,  0.0791,  0.9969,  0.0516,  0.9987, -0.4042,  0.9147,\n",
      "         -0.9927, -0.1205],\n",
      "        [-0.7568, -0.6536,  0.1054,  0.9944,  0.8296, -0.5584, -0.5268,  0.8500,\n",
      "         -0.1600,  0.9871],\n",
      "        [-0.9589,  0.2837,  0.1316,  0.9913, -0.9058, -0.4237, -0.6393,  0.7690,\n",
      "          0.9799,  0.1993],\n",
      "        [-0.2794,  0.9602,  0.1577,  0.9875,  0.1031,  0.9947, -0.7395,  0.6732,\n",
      "          0.2392, -0.9710],\n",
      "        [ 0.6570,  0.7539,  0.1837,  0.9830,  0.7997, -0.6005, -0.8254,  0.5645,\n",
      "         -0.9606, -0.2778],\n",
      "        [ 0.9894, -0.1455,  0.2095,  0.9778, -0.9265, -0.3764, -0.8955,  0.4450,\n",
      "         -0.3160,  0.9488],\n",
      "        [ 0.4121, -0.9111,  0.2353,  0.9719,  0.1543,  0.9880, -0.9485,  0.3168,\n",
      "          0.9354,  0.3536]])\n",
      "\n",
      "\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8415,  0.0264,  0.8573, -0.1383,  0.9992],\n",
      "        [ 0.9093,  0.0528, -0.8827, -0.2739,  0.0803],\n",
      "        [ 0.1411,  0.0791,  0.0516, -0.4042, -0.9927],\n",
      "        [-0.7568,  0.1054,  0.8296, -0.5268, -0.1600],\n",
      "        [-0.9589,  0.1316, -0.9058, -0.6393,  0.9799],\n",
      "        [-0.2794,  0.1577,  0.1031, -0.7395,  0.2392],\n",
      "        [ 0.6570,  0.1837,  0.7997, -0.8254, -0.9606],\n",
      "        [ 0.9894,  0.2095, -0.9265, -0.8955, -0.3160],\n",
      "        [ 0.4121,  0.2353,  0.1543, -0.9485,  0.9354]])\n",
      "\n",
      "\n",
      "tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.9997, -0.5148,  0.9904,  0.0402],\n",
      "        [-0.4161,  0.9986, -0.4699,  0.9618, -0.9968],\n",
      "        [-0.9900,  0.9969,  0.9987,  0.9147, -0.1205],\n",
      "        [-0.6536,  0.9944, -0.5584,  0.8500,  0.9871],\n",
      "        [ 0.2837,  0.9913, -0.4237,  0.7690,  0.1993],\n",
      "        [ 0.9602,  0.9875,  0.9947,  0.6732, -0.9710],\n",
      "        [ 0.7539,  0.9830, -0.6005,  0.5645, -0.2778],\n",
      "        [-0.1455,  0.9778, -0.3764,  0.4450,  0.9488],\n",
      "        [-0.9111,  0.9719,  0.9880,  0.3168,  0.3536]])\n"
     ]
    }
   ],
   "source": [
    "def dummyfn():\n",
    "    seq_len = 10\n",
    "    d_model = 10\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "    vector = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "    denominator_original = torch.pow(10000, vector/d_model)\n",
    "    denominator_harvard = torch.exp(vector * (-math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * denominator_original)\n",
    "    pe[:, 1::2] = torch.cos(position * denominator_original)\n",
    "    print(pe, pe[:, 0::2], pe[:, 1::2], sep='\\n\\n\\n')\n",
    "\n",
    "dummyfn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e824c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ec3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62782889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed882e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
